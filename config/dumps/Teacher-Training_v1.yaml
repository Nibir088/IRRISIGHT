defaults:
  - _self_  # Changed *self* to _self_ as it's the correct syntax
  
# config_dir: "/project/biocomplexity/wyr6fx(Nibir)/IJCAI-25_Irrigation_Mapping/Pytorch-Lightening/KIIM/config/multigpu-state-training-baseline-Nibir.yaml"

general:
  num_classes: 2
  learning_rate: 1e-4
eval:
  checkpoint_path: "/project/biocomplexity/wyr6fx(Nibir)/IrrigationMapping/baseline/wo_UT/Supervised/80/result_stats/checkpoints/epoch=2-val_iou_macro_irr=0.774.ckpt"
train:
  seed: 88
  # save_dir: "experiments_baseline_CO"
  save_model: true
  max_epochs: 50
  # batch_size: 16
  learning_rate: ${general.learning_rate}
  weight_decay: 1e-4
  early_stopping: true
  patience: 20
  accelerator: "gpu"
  devices: [0,1,2,3]
  strategy: "ddp"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4
  check_val_every_n_epoch: 1
  precision: 16
  mode: "max"
  monitor: "val_iou_macro_irr"
  verbose: true
  top_k: 1 #how checkpoint value to save
  alpha: 1 #0.6
  alpha_decay: 1 # 0.99
  
  
dataset:
  data_file_base_name: "/project/biocomplexity/wyr6fx(Nibir)/KDD-25-Data-Track/train_test_split/Sentinel/30p_wo_UT_combined.yaml"
  crop_matrices_path: "/project/biocomplexity/wyr6fx(Nibir)/IJCAI-25_Irrigation_Mapping/Pytorch-Lightening/Data-Collection/matrix"
  data_file_index: "UT"
  states: ["UT"]  # Added quotes for consistency
  image_size: [224, 224]  # Changed tuple notation to list
  transform: false
  gamma_value: 1.5
  image_types: ["image"]  # Added quotes for consistency
  agri_indices: ["ndvi", "ndti", "ndwi",'evi','gndvi','savi','msavi','rvi','cigreen','pri','osavi','wdrvi']  # Added quotes and spaces after commas
  source: "sentinel"
  train_size: 80
  is_supervised: true
  
dataloader:
  batch_size: 16
  num_workers: 8
  pin_memory: true
  
teachermodel:
  target: 'land_mask'
  BaseModel:
    use_pretrained_module: true
    use_attention_module: false
    use_multimodal_imagery_module: true
    use_projection_module: false
    num_classes: ${general.num_classes}
    loss_config:
      ce_weight: 1.0
      dice_weight: 0.0
      focal_weight: 0.0
      kg_weight: 0.0
    
  PretrainedModule:
    model_name: 'swin'
    in_channels: 7 ## sum of MultimodalImageryModule channel
    num_classes: ${general.num_classes} # ${model.BaseModel.num_classes}
    hidden_dim: 7  ### doesn't matter
    encoder_name: "resnet34"  ## doesn't matter
    encoder_weights: "imagenet" ## doesn't matter
    encoder_depth: 5 ## doesn't matter
    decoder_attention_type: 'None' ## doesn't matter
    activation: 'None' ## doesn't matter
    weights: 'landsat' ## doesn't matter
    pretrained: true 
    attention_type: "cross"  ## possible options self, cross, stream, none
    task: "segmentation"    ## segmentation or classification
  AttentionModule:
    in_channels: 1
    hidden_dim: 16
  MultimodalImageryModule:
    use_rgb: true
    use_ndvi: true
    use_ndwi: true
    use_ndti: false
    use_evi: false
    use_gndvi: false
    use_savi: true
    use_msavi: true
    use_rvi: false
    use_cigreen: false
    use_pri: false
    use_osavi: false
    use_wdrvi: false
    use_vegetation: true
  ProjectionModule:
    num_classes: ${general.num_classes} #${model.BaseModel.num_classes}
    in_channels: 21
    use_raw_spatial_priors: false
  # PretrainedBackbone:
    
    



# model:
#   backbone_name: "swin"
#   encoder_name: "resnet50"
#   num_classes: 4
#   learning_rate: 1e-4
#   use_attention: false
#   use_projection: false
#   use_rgb: true
#   use_ndvi: true
#   use_ndwi: true
#   use_ndti: true
#   pretrained_hidden_dim: 16
#   attention_hidden_dim: 16
#   gamma: 5.0
#   weight_decay: 1e-4
#   attention_type: "cross"
#   loss_config:
#     use_ce: false
#     ce_weight: 0.0      # Fixed indentation and removed curly braces
#     dice_weight: 0.4
#     focal_weight: 0.6
#     kg_weight: 0.0
#     stream_weight: 0.0
        
logging:
  use_wandb: true
  project_name: "irrigation-segmentation (IJCAI)"
  run_name: result_stats
  save_dir: "logs"
  
finetune:
  checkpoint_path: "experiments/best-model/model.ckpt"  # Path to pretrained model
  strict: false  # Whether to strictly enforce matching keys when loading
  freeze_backbone: false  # Whether to freeze backbone layers
  freeze_encoder: false # Whether to freeze encoder layers
  learning_rate: 1e-5  # Special learning rate for fine-tuning
  
      
hparam_tuning:
  enabled: true  # Set to true to enable hyperparameter tuning
  n_trials: 5   # Number of trials to run
  timeout_hours: 72  # Maximum duration for optimization (optional)
  search_space:
    learning_rate: [1e-4, 2e-4, 1e-3]
    weight_decay:
      min: 1e-6
      max: 1e-4
    dropout_rate:
      min: 0.1
      max: 0.5
    hidden_size:
      min: 32
      max: 32
      step: 0
    num_layers:
      min: 2
      max: 6
    batch_size: [16,32]
      
hydra:
  job:
    chdir: True
  run:
      dir: /project/biocomplexity/wyr6fx(Nibir)/IrrigationMapping/baseline_${teachermodel.target}/wo_${dataset.data_file_index}/Supervised/${dataset.train_size}
      # dir: /project/biocomplexity/wyr6fx(Nibir)/IJCAI-25_Irrigation_Mapping/Pytorch-Lightening/temp/test
    # dir: /project/biocomplexity/wyr6fx(Nibir)/IJCAI-25_Irrigation_Mapping/Pytorch-Lightening/outputs-KIIM-StateWise/${dataset.data_file_index}/2025-01-29/03-36-28
    # dir: /project/biocomplexity/wyr6fx(Nibir)/IJCAI-25_Irrigation_Mapping/Pytorch-Lightening/temp